{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8710419-e5a7-433d-b1d6-186a7ebddcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\hhars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a94a77aa-2610-41e0-af1f-64770666a431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adventure 69342\n",
      "belles_lettres 173096\n",
      "editorial 61604\n",
      "fiction 68488\n",
      "government 70117\n",
      "hobbies 82345\n",
      "humor 21695\n",
      "learned 181888\n",
      "lore 110299\n",
      "mystery 57169\n",
      "news 100554\n",
      "religion 39399\n",
      "reviews 40704\n",
      "romance 70022\n",
      "science_fiction 14470\n"
     ]
    }
   ],
   "source": [
    "# Get categories (genres) in the Brown Corpus\n",
    "categories = brown.categories()\n",
    "for c in categories:\n",
    "    print(c, end=\" \")\n",
    "    print(len(brown.words(categories=c)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62c59aae-93dc-47cb-84b9-4159746ae959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "038623da-2801-4701-a9b7-38b37a2c4a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hhars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e16625e-241c-49ba-bfcc-a5753369a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "document = \"\"\"In the quiet town nestled between rolling hills and meandering streams, life unfolded at a leisurely pace. The cobblestone streets led to charming cafes where locals gathered to share stories over cups of steaming coffee. Towering oak trees shaded the parks, their leaves rustling in the gentle breeze. \n",
    "\"\"\"\n",
    "sentence = \" It was a place where $ 40,000 $40,000 time seemed 1,2,3 23.23.32.233 to slow.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "db81e53e-8ba4-4f68-a987-8c69633e0210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the quiet town nestled between rolling hills and meandering streams, life unfolded at a leisurely pace.',\n",
       " 'The cobblestone streets led to charming cafes where locals gathered to share stories over cups of steaming coffee.',\n",
       " 'Towering oak trees shaded the parks, their leaves rustling in the gentle breeze.']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = sent_tokenize(document)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e5c3e5e-2503-4120-9d65-89213584a6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'was',\n",
       " 'a',\n",
       " 'place',\n",
       " 'where',\n",
       " '$',\n",
       " '40,000',\n",
       " '$',\n",
       " '40,000',\n",
       " 'time',\n",
       " 'seemed',\n",
       " '1,2,3',\n",
       " '23.23.32.233',\n",
       " 'to',\n",
       " 'slow',\n",
       " '.']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fee61820-e736-49ea-bd35-4a706fcf9702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hhars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "edddabdf-1016-4568-876d-ff6fef5d51a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ea5289b0-3874-4cf5-8f00-0b6ede5c6ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "11bde8f7-bf54-4d82-bf11-d04bec12acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_removal(text, stop_words):\n",
    "    useful_words = [w for w in text if w not in stop_words or w == 'not']\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "05897998-935e-42a8-9e5b-a37b8e31613a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'not', 'much', 'bothered', 'her.']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = 'I am not very much bothered about her.'.split()\n",
    "x = stopword_removal(words, stop_words)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "182fc09b-cb85-40d0-8968-68bcdd0c5597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hharshit8118@gmail.com',\n",
       " 'contact@sub1.sub2.example.org',\n",
       " 'jane_doe@example.org',\n",
       " 'user123@example.net',\n",
       " 'mary-jane@example.co',\n",
       " 'info@sub.example.com',\n",
       " 'john.doe@example.com']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'It was a place where time seemed to slow hharshit8118@gmail.com your marks are 59.34 and cost is $12,223 contact@sub1.sub2.example.org h@.d jane_doe@example.org user123@example.net mary-jane@example.co info@sub.example.com john.doe@example.com'\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer('[a-z.0-9_-]+@[a-z0-9._]+\\.[a-z]+')\n",
    "emails = tokenizer.tokenize(text)\n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b0173188-3b99-4cc5-b532-d18c71040b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5d31c3a5-9aac-42cd-ae94-92fb2f32d02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hhars\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "534c6569-ed95-44ee-961f-54aa26880b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "ss = SnowballStemmer('english')\n",
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "318c9374-25e5-45fa-84b5-daa794ff7806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porter jumping - jump\n",
      "snowball jumping - jump\n",
      "lancaster jumping - jump\n",
      "porter jumps - jump\n",
      "snowball jumps - jump\n",
      "lancaster jumps - jump\n",
      "porter jumped - jump\n",
      "snowball jumped - jump\n",
      "lancaster jumped - jump\n",
      "porter jump - jump\n",
      "snowball jump - jump\n",
      "lancaster jump - jump\n",
      "porter Programming - program\n",
      "snowball Programming - program\n",
      "lancaster Programming - program\n",
      "porter program - program\n",
      "snowball program - program\n",
      "lancaster program - program\n",
      "porter programs - program\n",
      "snowball programs - program\n",
      "lancaster programs - program\n",
      "porter Programmes - programm\n",
      "snowball Programmes - programm\n",
      "lancaster Programmes - program\n"
     ]
    }
   ],
   "source": [
    "listArr = [\"jumping\", \"jumps\", \"jumped\", \"jump\", \"Programming\", \"program\", \"programs\", \"Programmes\"]\n",
    "for w in listArr:\n",
    "    print(f'porter {w} - {ps.stem(w)}')\n",
    "    print(f'snowball {w} - {ss.stem(w)}')\n",
    "    print(f'lancaster {w} - {ls.stem(w)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ad5173dd-b0b9-47b3-bf34-8a7598f6d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Within the confines of a sunlit studio, an artist's creative spirit comes to life. The room is adorned with vibrant canvases, each telling a unique story born from the depths of imagination. Brushes dance across palettes, blending colors with purpose and passion. The scent of oil paint lingers in the air, a testament to the artist's dedication to the craft. The studio is a sanctuary where ideas flow freely, and every stroke on canvas is a manifestation of an inspired journey, inviting observers to glimpse into the soul of the artist and the world that unfolds within the realm of creativity.\",\n",
    "    'Cobblestone streets wind their way through a town steeped in history, each worn stone telling a tale of bygone eras. Ancient buildings with intricate facades stand as guardians of the past, while narrow alleyways invite exploration. The echoes of centuries-old footsteps resonate in the air, whispering stories of triumphs and tribulations. A visit to the town is a journey through time, as every corner reveals a piece of history waiting to be uncovered by those who walk its hallowed streets.',\n",
    "    'In the heart of the bustling city, a vibrant market comes alive every weekend, showcasing an array of culinary delights. The air is infused with the aroma of exotic spices, freshly baked bread, and simmering stews. Street vendors offer a kaleidoscope of flavors, from spicy street tacos to fragrant bowls of noodle soup. The market is a mosaic of cultures, where locals and tourists alike embark on a gastronomic journey, savoring the rich tapestry of global cuisine that fills the air with tantalizing scents and promises of culinary adventure.',\n",
    "    'In the heart of the countryside lies a tranquil meadow, where wildflowers sway in the soft breeze, painting the landscape with hues of violet, gold, and azure. The air is filled with the sweet melody of chirping birds, and the gentle rustle of leaves adds a rhythmic undertone. A meandering stream cuts through the meadow, providing a peaceful soundtrack to this idyllic scene. As the sun sets, casting a warm glow over the meadow, it becomes a haven for those seeking solace and connection with the untouched beauty of nature.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9f7d36f1-6650-4c11-9363-d65cba0970aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5c56e659-76a2-4803-b31b-b4a6d5805fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3870ecc9-dd57-4faf-9c68-9efac3e6c00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6c0b757d-2dd7-4412-bf72-3c8a7e42a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "99b93a4f-89d9-41eb-82dc-b793a8969494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e9e89103-c404-4473-8e50-117d9615f08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within 211 2\n",
      "within 211 0\n",
      "within 211 0\n",
      "within 211 0\n",
      "the 178 12\n",
      "the 178 4\n",
      "the 178 7\n",
      "the 178 11\n",
      "confines 40 1\n",
      "confines 40 0\n",
      "confines 40 0\n",
      "confines 40 0\n",
      "of 116 6\n",
      "of 116 5\n",
      "of 116 8\n",
      "of 116 5\n",
      "sunlit 168 1\n",
      "sunlit 168 0\n",
      "sunlit 168 0\n",
      "sunlit 168 0\n",
      "studio 166 2\n",
      "studio 166 0\n",
      "studio 166 0\n",
      "studio 166 0\n",
      "an 8 2\n",
      "an 8 0\n",
      "an 8 1\n",
      "an 8 0\n",
      "artist 13 3\n",
      "artist 13 0\n",
      "artist 13 0\n",
      "artist 13 0\n",
      "creative 45 1\n",
      "creative 45 0\n",
      "creative 45 0\n",
      "creative 45 0\n",
      "spirit 155 1\n",
      "spirit 155 0\n",
      "spirit 155 0\n",
      "spirit 155 0\n",
      "comes 39 1\n",
      "comes 39 0\n",
      "comes 39 1\n",
      "comes 39 0\n",
      "to 184 4\n",
      "to 184 2\n",
      "to 184 1\n",
      "to 184 1\n",
      "life 103 1\n",
      "life 103 0\n",
      "life 103 0\n",
      "life 103 0\n",
      "room 137 1\n",
      "room 137 0\n",
      "room 137 0\n",
      "room 137 0\n",
      "is 95 3\n",
      "is 95 1\n",
      "is 95 2\n",
      "is 95 1\n",
      "adorned 2 1\n",
      "adorned 2 0\n",
      "adorned 2 0\n",
      "adorned 2 0\n",
      "with 210 2\n",
      "with 210 1\n",
      "with 210 2\n",
      "with 210 3\n",
      "vibrant 196 1\n",
      "vibrant 196 0\n",
      "vibrant 196 1\n",
      "vibrant 196 0\n",
      "canvases 32 1\n",
      "canvases 32 0\n",
      "canvases 32 0\n",
      "canvases 32 0\n",
      "each 55 1\n",
      "each 55 1\n",
      "each 55 0\n",
      "each 55 0\n",
      "telling 175 1\n",
      "telling 175 1\n",
      "telling 175 0\n",
      "telling 175 0\n",
      "unique 193 1\n",
      "unique 193 0\n",
      "unique 193 0\n",
      "unique 193 0\n",
      "story 161 1\n",
      "story 161 0\n",
      "story 161 0\n",
      "story 161 0\n",
      "born 22 1\n",
      "born 22 0\n",
      "born 22 0\n",
      "born 22 0\n",
      "from 72 1\n",
      "from 72 0\n",
      "from 72 1\n",
      "from 72 0\n",
      "depths 54 1\n",
      "depths 54 0\n",
      "depths 54 0\n",
      "depths 54 0\n",
      "imagination 87 1\n",
      "imagination 87 0\n",
      "imagination 87 0\n",
      "imagination 87 0\n",
      "brushes 26 1\n",
      "brushes 26 0\n",
      "brushes 26 0\n",
      "brushes 26 0\n",
      "dance 51 1\n",
      "dance 51 0\n",
      "dance 51 0\n",
      "dance 51 0\n",
      "across 0 1\n",
      "across 0 0\n",
      "across 0 0\n",
      "across 0 0\n",
      "palettes 124 1\n",
      "palettes 124 0\n",
      "palettes 124 0\n",
      "palettes 124 0\n",
      "blending 21 1\n",
      "blending 21 0\n",
      "blending 21 0\n",
      "blending 21 0\n",
      "colors 38 1\n",
      "colors 38 0\n",
      "colors 38 0\n",
      "colors 38 0\n",
      "purpose 131 1\n",
      "purpose 131 0\n",
      "purpose 131 0\n",
      "purpose 131 0\n",
      "and 10 3\n",
      "and 10 1\n",
      "and 10 3\n",
      "and 10 3\n",
      "passion 125 1\n",
      "passion 125 0\n",
      "passion 125 0\n",
      "passion 125 0\n",
      "scent 142 1\n",
      "scent 142 0\n",
      "scent 142 0\n",
      "scent 142 0\n",
      "oil 118 1\n",
      "oil 118 0\n",
      "oil 118 0\n",
      "oil 118 0\n",
      "paint 122 1\n",
      "paint 122 0\n",
      "paint 122 0\n",
      "paint 122 0\n",
      "lingers 104 1\n",
      "lingers 104 0\n",
      "lingers 104 0\n",
      "lingers 104 0\n",
      "in 88 1\n",
      "in 88 2\n",
      "in 88 1\n",
      "in 88 2\n",
      "air 4 1\n",
      "air 4 1\n",
      "air 4 2\n",
      "air 4 1\n",
      "testament 176 1\n",
      "testament 176 0\n",
      "testament 176 0\n",
      "testament 176 0\n",
      "dedication 52 1\n",
      "dedication 52 0\n",
      "dedication 52 0\n",
      "dedication 52 0\n",
      "craft 44 1\n",
      "craft 44 0\n",
      "craft 44 0\n",
      "craft 44 0\n",
      "sanctuary 139 1\n",
      "sanctuary 139 0\n",
      "sanctuary 139 0\n",
      "sanctuary 139 0\n",
      "where 204 1\n",
      "where 204 0\n",
      "where 204 1\n",
      "where 204 1\n",
      "ideas 85 1\n",
      "ideas 85 0\n",
      "ideas 85 0\n",
      "ideas 85 0\n",
      "flow 66 1\n",
      "flow 66 0\n",
      "flow 66 0\n",
      "flow 66 0\n",
      "freely 70 1\n",
      "freely 70 0\n",
      "freely 70 0\n",
      "freely 70 0\n",
      "every 59 1\n",
      "every 59 1\n",
      "every 59 1\n",
      "every 59 0\n",
      "stroke 165 1\n",
      "stroke 165 0\n",
      "stroke 165 0\n",
      "stroke 165 0\n",
      "on 120 1\n",
      "on 120 0\n",
      "on 120 1\n",
      "on 120 0\n",
      "canvas 31 1\n",
      "canvas 31 0\n",
      "canvas 31 0\n",
      "canvas 31 0\n",
      "manifestation 106 1\n",
      "manifestation 106 0\n",
      "manifestation 106 0\n",
      "manifestation 106 0\n",
      "inspired 90 1\n",
      "inspired 90 0\n",
      "inspired 90 0\n",
      "inspired 90 0\n",
      "journey 98 1\n",
      "journey 98 1\n",
      "journey 98 1\n",
      "journey 98 0\n",
      "inviting 94 1\n",
      "inviting 94 0\n",
      "inviting 94 0\n",
      "inviting 94 0\n",
      "observers 115 1\n",
      "observers 115 0\n",
      "observers 115 0\n",
      "observers 115 0\n",
      "glimpse 75 1\n",
      "glimpse 75 0\n",
      "glimpse 75 0\n",
      "glimpse 75 0\n",
      "into 91 1\n",
      "into 91 0\n",
      "into 91 0\n",
      "into 91 0\n",
      "soul 150 1\n",
      "soul 150 0\n",
      "soul 150 0\n",
      "soul 150 0\n",
      "world 212 1\n",
      "world 212 0\n",
      "world 212 0\n",
      "world 212 0\n",
      "that 177 1\n",
      "that 177 0\n",
      "that 177 1\n",
      "that 177 0\n",
      "unfolds 192 1\n",
      "unfolds 192 0\n",
      "unfolds 192 0\n",
      "unfolds 192 0\n",
      "realm 132 1\n",
      "realm 132 0\n",
      "realm 132 0\n",
      "realm 132 0\n",
      "creativity 46 1\n",
      "creativity 46 0\n",
      "creativity 46 0\n",
      "creativity 46 0\n",
      "cobblestone 37 0\n",
      "cobblestone 37 1\n",
      "cobblestone 37 0\n",
      "cobblestone 37 0\n",
      "streets 164 0\n",
      "streets 164 2\n",
      "streets 164 0\n",
      "streets 164 0\n",
      "wind 209 0\n",
      "wind 209 1\n",
      "wind 209 0\n",
      "wind 209 0\n",
      "their 179 0\n",
      "their 179 1\n",
      "their 179 0\n",
      "their 179 0\n",
      "way 202 0\n",
      "way 202 1\n",
      "way 202 0\n",
      "way 202 0\n",
      "through 182 0\n",
      "through 182 2\n",
      "through 182 0\n",
      "through 182 1\n",
      "town 186 0\n",
      "town 186 2\n",
      "town 186 0\n",
      "town 186 0\n",
      "steeped 157 0\n",
      "steeped 157 1\n",
      "steeped 157 0\n",
      "steeped 157 0\n",
      "history 83 0\n",
      "history 83 2\n",
      "history 83 0\n",
      "history 83 0\n",
      "worn 213 0\n",
      "worn 213 1\n",
      "worn 213 0\n",
      "worn 213 0\n",
      "stone 159 0\n",
      "stone 159 1\n",
      "stone 159 0\n",
      "stone 159 0\n",
      "tale 172 0\n",
      "tale 172 1\n",
      "tale 172 0\n",
      "tale 172 0\n",
      "bygone 30 0\n",
      "bygone 30 1\n",
      "bygone 30 0\n",
      "bygone 30 0\n",
      "eras 58 0\n",
      "eras 58 1\n",
      "eras 58 0\n",
      "eras 58 0\n",
      "ancient 9 0\n",
      "ancient 9 1\n",
      "ancient 9 0\n",
      "ancient 9 0\n",
      "buildings 27 0\n",
      "buildings 27 1\n",
      "buildings 27 0\n",
      "buildings 27 0\n",
      "intricate 92 0\n",
      "intricate 92 1\n",
      "intricate 92 0\n",
      "intricate 92 0\n",
      "facades 62 0\n",
      "facades 62 1\n",
      "facades 62 0\n",
      "facades 62 0\n",
      "stand 156 0\n",
      "stand 156 1\n",
      "stand 156 0\n",
      "stand 156 0\n",
      "as 14 0\n",
      "as 14 2\n",
      "as 14 0\n",
      "as 14 1\n",
      "guardians 79 0\n",
      "guardians 79 1\n",
      "guardians 79 0\n",
      "guardians 79 0\n",
      "past 126 0\n",
      "past 126 1\n",
      "past 126 0\n",
      "past 126 0\n",
      "while 205 0\n",
      "while 205 1\n",
      "while 205 0\n",
      "while 205 0\n",
      "narrow 112 0\n",
      "narrow 112 1\n",
      "narrow 112 0\n",
      "narrow 112 0\n",
      "alleyways 7 0\n",
      "alleyways 7 1\n",
      "alleyways 7 0\n",
      "alleyways 7 0\n",
      "invite 93 0\n",
      "invite 93 1\n",
      "invite 93 0\n",
      "invite 93 0\n",
      "exploration 61 0\n",
      "exploration 61 1\n",
      "exploration 61 0\n",
      "exploration 61 0\n",
      "echoes 56 0\n",
      "echoes 56 1\n",
      "echoes 56 0\n",
      "echoes 56 0\n",
      "centuries 34 0\n",
      "centuries 34 1\n",
      "centuries 34 0\n",
      "centuries 34 0\n",
      "old 119 0\n",
      "old 119 1\n",
      "old 119 0\n",
      "old 119 0\n",
      "footsteps 67 0\n",
      "footsteps 67 1\n",
      "footsteps 67 0\n",
      "footsteps 67 0\n",
      "resonate 133 0\n",
      "resonate 133 1\n",
      "resonate 133 0\n",
      "resonate 133 0\n",
      "whispering 206 0\n",
      "whispering 206 1\n",
      "whispering 206 0\n",
      "whispering 206 0\n",
      "stories 160 0\n",
      "stories 160 1\n",
      "stories 160 0\n",
      "stories 160 0\n",
      "triumphs 189 0\n",
      "triumphs 189 1\n",
      "triumphs 189 0\n",
      "triumphs 189 0\n",
      "tribulations 188 0\n",
      "tribulations 188 1\n",
      "tribulations 188 0\n",
      "tribulations 188 0\n",
      "visit 198 0\n",
      "visit 198 1\n",
      "visit 198 0\n",
      "visit 198 0\n",
      "time 183 0\n",
      "time 183 1\n",
      "time 183 0\n",
      "time 183 0\n",
      "corner 42 0\n",
      "corner 42 1\n",
      "corner 42 0\n",
      "corner 42 0\n",
      "reveals 134 0\n",
      "reveals 134 1\n",
      "reveals 134 0\n",
      "reveals 134 0\n",
      "piece 128 0\n",
      "piece 128 1\n",
      "piece 128 0\n",
      "piece 128 0\n",
      "waiting 199 0\n",
      "waiting 199 1\n",
      "waiting 199 0\n",
      "waiting 199 0\n",
      "be 17 0\n",
      "be 17 1\n",
      "be 17 0\n",
      "be 17 0\n",
      "uncovered 190 0\n",
      "uncovered 190 1\n",
      "uncovered 190 0\n",
      "uncovered 190 0\n",
      "by 29 0\n",
      "by 29 1\n",
      "by 29 0\n",
      "by 29 0\n",
      "those 181 0\n",
      "those 181 1\n",
      "those 181 0\n",
      "those 181 1\n",
      "who 207 0\n",
      "who 207 1\n",
      "who 207 0\n",
      "who 207 0\n",
      "walk 200 0\n",
      "walk 200 1\n",
      "walk 200 0\n",
      "walk 200 0\n",
      "its 97 0\n",
      "its 97 1\n",
      "its 97 0\n",
      "its 97 0\n",
      "hallowed 80 0\n",
      "hallowed 80 1\n",
      "hallowed 80 0\n",
      "hallowed 80 0\n",
      "heart 82 0\n",
      "heart 82 0\n",
      "heart 82 1\n",
      "heart 82 1\n",
      "bustling 28 0\n",
      "bustling 28 0\n",
      "bustling 28 1\n",
      "bustling 28 0\n",
      "city 36 0\n",
      "city 36 0\n",
      "city 36 1\n",
      "city 36 0\n",
      "market 107 0\n",
      "market 107 0\n",
      "market 107 2\n",
      "market 107 0\n",
      "alive 6 0\n",
      "alive 6 0\n",
      "alive 6 1\n",
      "alive 6 0\n",
      "weekend 203 0\n",
      "weekend 203 0\n",
      "weekend 203 1\n",
      "weekend 203 0\n",
      "showcasing 146 0\n",
      "showcasing 146 0\n",
      "showcasing 146 1\n",
      "showcasing 146 0\n",
      "array 12 0\n",
      "array 12 0\n",
      "array 12 1\n",
      "array 12 0\n",
      "culinary 48 0\n",
      "culinary 48 0\n",
      "culinary 48 2\n",
      "culinary 48 0\n",
      "delights 53 0\n",
      "delights 53 0\n",
      "delights 53 1\n",
      "delights 53 0\n",
      "infused 89 0\n",
      "infused 89 0\n",
      "infused 89 1\n",
      "infused 89 0\n",
      "aroma 11 0\n",
      "aroma 11 0\n",
      "aroma 11 1\n",
      "aroma 11 0\n",
      "exotic 60 0\n",
      "exotic 60 0\n",
      "exotic 60 1\n",
      "exotic 60 0\n",
      "spices 153 0\n",
      "spices 153 0\n",
      "spices 153 1\n",
      "spices 153 0\n",
      "freshly 71 0\n",
      "freshly 71 0\n",
      "freshly 71 1\n",
      "freshly 71 0\n",
      "baked 16 0\n",
      "baked 16 0\n",
      "baked 16 1\n",
      "baked 16 0\n",
      "bread 24 0\n",
      "bread 24 0\n",
      "bread 24 1\n",
      "bread 24 0\n",
      "simmering 147 0\n",
      "simmering 147 0\n",
      "simmering 147 1\n",
      "simmering 147 0\n",
      "stews 158 0\n",
      "stews 158 0\n",
      "stews 158 1\n",
      "stews 158 0\n",
      "street 163 0\n",
      "street 163 0\n",
      "street 163 2\n",
      "street 163 0\n",
      "vendors 195 0\n",
      "vendors 195 0\n",
      "vendors 195 1\n",
      "vendors 195 0\n",
      "offer 117 0\n",
      "offer 117 0\n",
      "offer 117 1\n",
      "offer 117 0\n",
      "kaleidoscope 99 0\n",
      "kaleidoscope 99 0\n",
      "kaleidoscope 99 1\n",
      "kaleidoscope 99 0\n",
      "flavors 65 0\n",
      "flavors 65 0\n",
      "flavors 65 1\n",
      "flavors 65 0\n",
      "spicy 154 0\n",
      "spicy 154 0\n",
      "spicy 154 1\n",
      "spicy 154 0\n",
      "tacos 171 0\n",
      "tacos 171 0\n",
      "tacos 171 1\n",
      "tacos 171 0\n",
      "fragrant 69 0\n",
      "fragrant 69 0\n",
      "fragrant 69 1\n",
      "fragrant 69 0\n",
      "bowls 23 0\n",
      "bowls 23 0\n",
      "bowls 23 1\n",
      "bowls 23 0\n",
      "noodle 114 0\n",
      "noodle 114 0\n",
      "noodle 114 1\n",
      "noodle 114 0\n",
      "soup 152 0\n",
      "soup 152 0\n",
      "soup 152 1\n",
      "soup 152 0\n",
      "mosaic 111 0\n",
      "mosaic 111 0\n",
      "mosaic 111 1\n",
      "mosaic 111 0\n",
      "cultures 49 0\n",
      "cultures 49 0\n",
      "cultures 49 1\n",
      "cultures 49 0\n",
      "locals 105 0\n",
      "locals 105 0\n",
      "locals 105 1\n",
      "locals 105 0\n",
      "tourists 185 0\n",
      "tourists 185 0\n",
      "tourists 185 1\n",
      "tourists 185 0\n",
      "alike 5 0\n",
      "alike 5 0\n",
      "alike 5 1\n",
      "alike 5 0\n",
      "embark 57 0\n",
      "embark 57 0\n",
      "embark 57 1\n",
      "embark 57 0\n",
      "gastronomic 73 0\n",
      "gastronomic 73 0\n",
      "gastronomic 73 1\n",
      "gastronomic 73 0\n",
      "savoring 140 0\n",
      "savoring 140 0\n",
      "savoring 140 1\n",
      "savoring 140 0\n",
      "rich 136 0\n",
      "rich 136 0\n",
      "rich 136 1\n",
      "rich 136 0\n",
      "tapestry 174 0\n",
      "tapestry 174 0\n",
      "tapestry 174 1\n",
      "tapestry 174 0\n",
      "global 76 0\n",
      "global 76 0\n",
      "global 76 1\n",
      "global 76 0\n",
      "cuisine 47 0\n",
      "cuisine 47 0\n",
      "cuisine 47 1\n",
      "cuisine 47 0\n",
      "fills 64 0\n",
      "fills 64 0\n",
      "fills 64 1\n",
      "fills 64 0\n",
      "tantalizing 173 0\n",
      "tantalizing 173 0\n",
      "tantalizing 173 1\n",
      "tantalizing 173 0\n",
      "scents 143 0\n",
      "scents 143 0\n",
      "scents 143 1\n",
      "scents 143 0\n",
      "promises 129 0\n",
      "promises 129 0\n",
      "promises 129 1\n",
      "promises 129 0\n",
      "adventure 3 0\n",
      "adventure 3 0\n",
      "adventure 3 1\n",
      "adventure 3 0\n",
      "countryside 43 0\n",
      "countryside 43 0\n",
      "countryside 43 0\n",
      "countryside 43 1\n",
      "lies 102 0\n",
      "lies 102 0\n",
      "lies 102 0\n",
      "lies 102 1\n",
      "tranquil 187 0\n",
      "tranquil 187 0\n",
      "tranquil 187 0\n",
      "tranquil 187 1\n",
      "meadow 108 0\n",
      "meadow 108 0\n",
      "meadow 108 0\n",
      "meadow 108 3\n",
      "wildflowers 208 0\n",
      "wildflowers 208 0\n",
      "wildflowers 208 0\n",
      "wildflowers 208 1\n",
      "sway 169 0\n",
      "sway 169 0\n",
      "sway 169 0\n",
      "sway 169 1\n",
      "soft 148 0\n",
      "soft 148 0\n",
      "soft 148 0\n",
      "soft 148 1\n",
      "breeze 25 0\n",
      "breeze 25 0\n",
      "breeze 25 0\n",
      "breeze 25 1\n",
      "painting 123 0\n",
      "painting 123 0\n",
      "painting 123 0\n",
      "painting 123 1\n",
      "landscape 100 0\n",
      "landscape 100 0\n",
      "landscape 100 0\n",
      "landscape 100 1\n",
      "hues 84 0\n",
      "hues 84 0\n",
      "hues 84 0\n",
      "hues 84 1\n",
      "violet 197 0\n",
      "violet 197 0\n",
      "violet 197 0\n",
      "violet 197 1\n",
      "gold 78 0\n",
      "gold 78 0\n",
      "gold 78 0\n",
      "gold 78 1\n",
      "azure 15 0\n",
      "azure 15 0\n",
      "azure 15 0\n",
      "azure 15 1\n",
      "filled 63 0\n",
      "filled 63 0\n",
      "filled 63 0\n",
      "filled 63 1\n",
      "sweet 170 0\n",
      "sweet 170 0\n",
      "sweet 170 0\n",
      "sweet 170 1\n",
      "melody 110 0\n",
      "melody 110 0\n",
      "melody 110 0\n",
      "melody 110 1\n",
      "chirping 35 0\n",
      "chirping 35 0\n",
      "chirping 35 0\n",
      "chirping 35 1\n",
      "birds 20 0\n",
      "birds 20 0\n",
      "birds 20 0\n",
      "birds 20 1\n",
      "gentle 74 0\n",
      "gentle 74 0\n",
      "gentle 74 0\n",
      "gentle 74 1\n",
      "rustle 138 0\n",
      "rustle 138 0\n",
      "rustle 138 0\n",
      "rustle 138 1\n",
      "leaves 101 0\n",
      "leaves 101 0\n",
      "leaves 101 0\n",
      "leaves 101 1\n",
      "adds 1 0\n",
      "adds 1 0\n",
      "adds 1 0\n",
      "adds 1 1\n",
      "rhythmic 135 0\n",
      "rhythmic 135 0\n",
      "rhythmic 135 0\n",
      "rhythmic 135 1\n",
      "undertone 191 0\n",
      "undertone 191 0\n",
      "undertone 191 0\n",
      "undertone 191 1\n",
      "meandering 109 0\n",
      "meandering 109 0\n",
      "meandering 109 0\n",
      "meandering 109 1\n",
      "stream 162 0\n",
      "stream 162 0\n",
      "stream 162 0\n",
      "stream 162 1\n",
      "cuts 50 0\n",
      "cuts 50 0\n",
      "cuts 50 0\n",
      "cuts 50 1\n",
      "providing 130 0\n",
      "providing 130 0\n",
      "providing 130 0\n",
      "providing 130 1\n",
      "peaceful 127 0\n",
      "peaceful 127 0\n",
      "peaceful 127 0\n",
      "peaceful 127 1\n",
      "soundtrack 151 0\n",
      "soundtrack 151 0\n",
      "soundtrack 151 0\n",
      "soundtrack 151 1\n",
      "this 180 0\n",
      "this 180 0\n",
      "this 180 0\n",
      "this 180 1\n",
      "idyllic 86 0\n",
      "idyllic 86 0\n",
      "idyllic 86 0\n",
      "idyllic 86 1\n",
      "scene 141 0\n",
      "scene 141 0\n",
      "scene 141 0\n",
      "scene 141 1\n",
      "sun 167 0\n",
      "sun 167 0\n",
      "sun 167 0\n",
      "sun 167 1\n",
      "sets 145 0\n",
      "sets 145 0\n",
      "sets 145 0\n",
      "sets 145 1\n",
      "casting 33 0\n",
      "casting 33 0\n",
      "casting 33 0\n",
      "casting 33 1\n",
      "warm 201 0\n",
      "warm 201 0\n",
      "warm 201 0\n",
      "warm 201 1\n",
      "glow 77 0\n",
      "glow 77 0\n",
      "glow 77 0\n",
      "glow 77 1\n",
      "over 121 0\n",
      "over 121 0\n",
      "over 121 0\n",
      "over 121 1\n",
      "it 96 0\n",
      "it 96 0\n",
      "it 96 0\n",
      "it 96 1\n",
      "becomes 19 0\n",
      "becomes 19 0\n",
      "becomes 19 0\n",
      "becomes 19 1\n",
      "haven 81 0\n",
      "haven 81 0\n",
      "haven 81 0\n",
      "haven 81 1\n",
      "for 68 0\n",
      "for 68 0\n",
      "for 68 0\n",
      "for 68 1\n",
      "seeking 144 0\n",
      "seeking 144 0\n",
      "seeking 144 0\n",
      "seeking 144 1\n",
      "solace 149 0\n",
      "solace 149 0\n",
      "solace 149 0\n",
      "solace 149 1\n",
      "connection 41 0\n",
      "connection 41 0\n",
      "connection 41 0\n",
      "connection 41 1\n",
      "untouched 194 0\n",
      "untouched 194 0\n",
      "untouched 194 0\n",
      "untouched 194 1\n",
      "beauty 18 0\n",
      "beauty 18 0\n",
      "beauty 18 0\n",
      "beauty 18 1\n",
      "nature 113 0\n",
      "nature 113 0\n",
      "nature 113 0\n",
      "nature 113 1\n"
     ]
    }
   ],
   "source": [
    "for (k, v) in cv.vocabulary_.items():\n",
    "    print(k, v, vectorized_corpus[0][v])\n",
    "    print(k, v, vectorized_corpus[1][v])\n",
    "    print(k, v, vectorized_corpus[2][v])\n",
    "    print(k, v, vectorized_corpus[3][v])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "507d8b70-5abe-4d95-afd0-c8727d2b9fee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 2, 1, 1, 0, 1, 0, 3, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 2, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 8, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 7, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "30db6f60-d445-4806-a31b-bf6ea286a9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = cv.inverse_transform(vectorized_corpus[2:3,:])\n",
    "len(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "5645f017-17a3-46cd-b9e0-d70bd13a5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('[a-zA-Z@.,0-9]+')\n",
    "def myTokenizer(document):\n",
    "    words = tokenizer.tokenize(document.lower())\n",
    "    words = stopword_removal(words, stop_words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9bd89304-e782-40b4-b528-d94db22910e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'was', 'a', 'place', 'where', 'time', 'seemed', 'to', 'slow', 'hharshit8118@gmail.com', 'your', 'marks', 'are', '59.34', 'and', 'cost', 'is', '12,223', 'contact@sub1.sub2.example.org', 'h@.d', 'jane', 'doe@example.org', 'user123@example.net', 'mary', 'jane@example.co', 'info@sub.example.com', 'john.doe@example.com']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['place',\n",
       " 'time',\n",
       " 'seemed',\n",
       " 'slow',\n",
       " 'hharshit8118@gmail.com',\n",
       " 'marks',\n",
       " '59.34',\n",
       " 'cost',\n",
       " '12,223',\n",
       " 'contact@sub1.sub2.example.org',\n",
       " 'h@.d',\n",
       " 'jane',\n",
       " 'doe@example.org',\n",
       " 'user123@example.net',\n",
       " 'mary',\n",
       " 'jane@example.co',\n",
       " 'info@sub.example.com',\n",
       " 'john.doe@example.com']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b5c6aa3b-969c-4ae3-88b5-d7a19bbfd746",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "62822a43-1076-4021-a845-7e99e180226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['within', 'the', 'confines', 'of', 'a', 'sunlit', 'studio,', 'an', 'artist', 's', 'creative', 'spirit', 'comes', 'to', 'life.', 'the', 'room', 'is', 'adorned', 'with', 'vibrant', 'canvases,', 'each', 'telling', 'a', 'unique', 'story', 'born', 'from', 'the', 'depths', 'of', 'imagination.', 'brushes', 'dance', 'across', 'palettes,', 'blending', 'colors', 'with', 'purpose', 'and', 'passion.', 'the', 'scent', 'of', 'oil', 'paint', 'lingers', 'in', 'the', 'air,', 'a', 'testament', 'to', 'the', 'artist', 's', 'dedication', 'to', 'the', 'craft.', 'the', 'studio', 'is', 'a', 'sanctuary', 'where', 'ideas', 'flow', 'freely,', 'and', 'every', 'stroke', 'on', 'canvas', 'is', 'a', 'manifestation', 'of', 'an', 'inspired', 'journey,', 'inviting', 'observers', 'to', 'glimpse', 'into', 'the', 'soul', 'of', 'the', 'artist', 'and', 'the', 'world', 'that', 'unfolds', 'within', 'the', 'realm', 'of', 'creativity.']\n",
      "['cobblestone', 'streets', 'wind', 'their', 'way', 'through', 'a', 'town', 'steeped', 'in', 'history,', 'each', 'worn', 'stone', 'telling', 'a', 'tale', 'of', 'bygone', 'eras.', 'ancient', 'buildings', 'with', 'intricate', 'facades', 'stand', 'as', 'guardians', 'of', 'the', 'past,', 'while', 'narrow', 'alleyways', 'invite', 'exploration.', 'the', 'echoes', 'of', 'centuries', 'old', 'footsteps', 'resonate', 'in', 'the', 'air,', 'whispering', 'stories', 'of', 'triumphs', 'and', 'tribulations.', 'a', 'visit', 'to', 'the', 'town', 'is', 'a', 'journey', 'through', 'time,', 'as', 'every', 'corner', 'reveals', 'a', 'piece', 'of', 'history', 'waiting', 'to', 'be', 'uncovered', 'by', 'those', 'who', 'walk', 'its', 'hallowed', 'streets.']\n",
      "['in', 'the', 'heart', 'of', 'the', 'bustling', 'city,', 'a', 'vibrant', 'market', 'comes', 'alive', 'every', 'weekend,', 'showcasing', 'an', 'array', 'of', 'culinary', 'delights.', 'the', 'air', 'is', 'infused', 'with', 'the', 'aroma', 'of', 'exotic', 'spices,', 'freshly', 'baked', 'bread,', 'and', 'simmering', 'stews.', 'street', 'vendors', 'offer', 'a', 'kaleidoscope', 'of', 'flavors,', 'from', 'spicy', 'street', 'tacos', 'to', 'fragrant', 'bowls', 'of', 'noodle', 'soup.', 'the', 'market', 'is', 'a', 'mosaic', 'of', 'cultures,', 'where', 'locals', 'and', 'tourists', 'alike', 'embark', 'on', 'a', 'gastronomic', 'journey,', 'savoring', 'the', 'rich', 'tapestry', 'of', 'global', 'cuisine', 'that', 'fills', 'the', 'air', 'with', 'tantalizing', 'scents', 'and', 'promises', 'of', 'culinary', 'adventure.']\n",
      "['in', 'the', 'heart', 'of', 'the', 'countryside', 'lies', 'a', 'tranquil', 'meadow,', 'where', 'wildflowers', 'sway', 'in', 'the', 'soft', 'breeze,', 'painting', 'the', 'landscape', 'with', 'hues', 'of', 'violet,', 'gold,', 'and', 'azure.', 'the', 'air', 'is', 'filled', 'with', 'the', 'sweet', 'melody', 'of', 'chirping', 'birds,', 'and', 'the', 'gentle', 'rustle', 'of', 'leaves', 'adds', 'a', 'rhythmic', 'undertone.', 'a', 'meandering', 'stream', 'cuts', 'through', 'the', 'meadow,', 'providing', 'a', 'peaceful', 'soundtrack', 'to', 'this', 'idyllic', 'scene.', 'as', 'the', 'sun', 'sets,', 'casting', 'a', 'warm', 'glow', 'over', 'the', 'meadow,', 'it', 'becomes', 'a', 'haven', 'for', 'those', 'seeking', 'solace', 'and', 'connection', 'with', 'the', 'untouched', 'beauty', 'of', 'nature.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 2, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        2, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 1, 1, 1, 0, 0, 0, 0, 0, 3, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus).toarray()\n",
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "94c28969-e3d6-4f3c-9d01-d46aec0b44a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9ff85231-0d86-457a-be00-50e4683d9418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['across', 'adorned', 'air,', 'artist', 'blending', 'born',\n",
       "        'brushes', 'canvas', 'canvases,', 'colors', 'comes', 'confines',\n",
       "        'craft.', 'creative', 'creativity.', 'dance', 'dedication',\n",
       "        'depths', 'every', 'flow', 'freely,', 'glimpse', 'ideas',\n",
       "        'imagination.', 'inspired', 'inviting', 'journey,', 'life.',\n",
       "        'lingers', 'manifestation', 'observers', 'oil', 'paint',\n",
       "        'palettes,', 'passion.', 'purpose', 'realm', 'room', 'sanctuary',\n",
       "        'scent', 'soul', 'spirit', 'story', 'stroke', 'studio', 'studio,',\n",
       "        'sunlit', 'telling', 'testament', 'unfolds', 'unique', 'vibrant',\n",
       "        'within', 'world'], dtype='<U13'),\n",
       " array(['air,', 'alleyways', 'ancient', 'buildings', 'bygone', 'centuries',\n",
       "        'cobblestone', 'corner', 'echoes', 'eras.', 'every',\n",
       "        'exploration.', 'facades', 'footsteps', 'guardians', 'hallowed',\n",
       "        'history', 'history,', 'intricate', 'invite', 'journey', 'narrow',\n",
       "        'old', 'past,', 'piece', 'resonate', 'reveals', 'stand', 'steeped',\n",
       "        'stone', 'stories', 'streets', 'streets.', 'tale', 'telling',\n",
       "        'time,', 'town', 'tribulations.', 'triumphs', 'uncovered', 'visit',\n",
       "        'waiting', 'walk', 'way', 'whispering', 'wind', 'worn'],\n",
       "       dtype='<U13'),\n",
       " array(['adventure.', 'air', 'alike', 'alive', 'aroma', 'array', 'baked',\n",
       "        'bowls', 'bread,', 'bustling', 'city,', 'comes', 'cuisine',\n",
       "        'culinary', 'cultures,', 'delights.', 'embark', 'every', 'exotic',\n",
       "        'fills', 'flavors,', 'fragrant', 'freshly', 'gastronomic',\n",
       "        'global', 'heart', 'infused', 'journey,', 'kaleidoscope', 'locals',\n",
       "        'market', 'mosaic', 'noodle', 'offer', 'promises', 'rich',\n",
       "        'savoring', 'scents', 'showcasing', 'simmering', 'soup.',\n",
       "        'spices,', 'spicy', 'stews.', 'street', 'tacos', 'tantalizing',\n",
       "        'tapestry', 'tourists', 'vendors', 'vibrant', 'weekend,'],\n",
       "       dtype='<U13'),\n",
       " array(['adds', 'air', 'azure.', 'beauty', 'becomes', 'birds,', 'breeze,',\n",
       "        'casting', 'chirping', 'connection', 'countryside', 'cuts',\n",
       "        'filled', 'gentle', 'glow', 'gold,', 'heart', 'hues', 'idyllic',\n",
       "        'landscape', 'leaves', 'lies', 'meadow,', 'meandering', 'melody',\n",
       "        'nature.', 'painting', 'peaceful', 'providing', 'rhythmic',\n",
       "        'rustle', 'scene.', 'seeking', 'sets,', 'soft', 'solace',\n",
       "        'soundtrack', 'stream', 'sun', 'sway', 'sweet', 'tranquil',\n",
       "        'undertone.', 'untouched', 'violet,', 'warm', 'wildflowers'],\n",
       "       dtype='<U13')]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = cv.inverse_transform(vectorized_corpus)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e493e48f-f8b5-4bbe-ad6b-6a642573d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = ['I love cricket untouched vibrant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "dd741250-3a5b-4367-beef-6039a831e905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'love', 'cricket', 'untouched', 'vibrant']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(test_corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d1e95382-387f-4c71-89e1-77f400d0a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"this is good movie\"\n",
    "sent2 = \"this is not good movie\"\n",
    "sent3 = \"this is good movie but actor was not good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "db5d6dea-9f35-4acb-895e-457b1ad00607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 1 0 1 0]\n",
      " [0 0 1 1 1 1 1 0]\n",
      " [1 1 2 1 1 1 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'this': 6,\n",
       " 'is': 3,\n",
       " 'good': 2,\n",
       " 'movie': 4,\n",
       " 'not': 5,\n",
       " 'but': 1,\n",
       " 'actor': 0,\n",
       " 'was': 7}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = [sent1, sent2, sent3]\n",
    "cv = CountVectorizer(ngram_range=(1,1))\n",
    "vectorized_corpus = cv.fit_transform(doc).toarray()\n",
    "print(vectorized_corpus)\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9dbc89f9-2a87-4c3b-864b-1b8f30f33c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 1 0 0 0 1 0]\n",
      " [0 0 1 0 1 0 1 1 0]\n",
      " [1 1 1 1 0 1 1 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'this is': 7,\n",
       " 'is good': 3,\n",
       " 'good movie': 2,\n",
       " 'is not': 4,\n",
       " 'not good': 6,\n",
       " 'movie but': 5,\n",
       " 'but actor': 1,\n",
       " 'actor was': 0,\n",
       " 'was not': 8}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "vectorized_corpus = cv.fit_transform(doc).toarray()\n",
    "print(vectorized_corpus)\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "68767690-2d2f-47ce-8ccd-71241008ee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'good', 'movie']\n",
      "['this', 'is', 'not', 'good', 'movie']\n",
      "['this', 'is', 'good', 'movie', 'but', 'actor', 'was', 'not', 'good']\n",
      "[[0 0 0 1 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 1 0 0 1 1 1]\n",
      " [1 1 1 2 1 1 1 1 1 1 1 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhars\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'good': 3,\n",
       " 'movie': 6,\n",
       " 'good movie': 4,\n",
       " 'not': 9,\n",
       " 'not good': 10,\n",
       " 'not good movie': 11,\n",
       " 'actor': 0,\n",
       " 'movie actor': 7,\n",
       " 'actor not': 1,\n",
       " 'good movie actor': 5,\n",
       " 'movie actor not': 8,\n",
       " 'actor not good': 2}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,3), tokenizer=myTokenizer)\n",
    "vectorized_corpus = cv.fit_transform(doc).toarray()\n",
    "print(vectorized_corpus)\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "40209420-d089-4f60-b06c-fe1760fbda2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['good', 'good movie', 'movie'], dtype='<U16'),\n",
       " array(['good', 'good movie', 'movie', 'not', 'not good', 'not good movie'],\n",
       "       dtype='<U16'),\n",
       " array(['actor', 'actor not', 'actor not good', 'good', 'good movie',\n",
       "        'good movie actor', 'movie', 'movie actor', 'movie actor not',\n",
       "        'not', 'not good'], dtype='<U16')]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308b972-a75c-4fe0-a2a5-e001836fcc48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c2cb3-52a6-411d-b191-a4b6571a4cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
